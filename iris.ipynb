{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c78a7069",
   "metadata": {},
   "source": [
    "# KMeans Clustering Iris Data\n",
    "\n",
    "By Leo Spratt, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314af45e-f2bd-4e42-acbb-cedbae9f53f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "I have chosen to use the Iris flower dataset to illustrate K-Means clustering as it contains several attributes that are given directly as numbers in centimetres, which was created by RA Fisher and Dr E. Anderson 1936.\n",
    "\n",
    "From my own observations in my garden I have seen that Irises have a very distinct petal and sepal arrangement which would surely make them easy to measure accurately. When fully open the irises petal and sepal are the same color.\n",
    "\n",
    "There are approximately \"300 species\" of irises (Britannica, 2021), most of which are visually similar and hard to distinguish apart.\n",
    "\n",
    "Fisher's dataset took only three different Iris species (Setosa, Virginica and Versicolor). He used measurements of the petals and sepals of these three species to make a multivariate dataset. These figures are often used for machine learning demos and experiments as the data is linearly separable and has no empty columns/rows.\n",
    "\n",
    "For example the iris dataset has been used in combination with a wine dataset for a study on the performance of K-Means with a different number of attributes (Bora et al, 2014). Another example is comparing different distance metrics to estimate cluster values (Chakraborty et al, 2020).\n",
    "\n",
    "There are several methods of clustering data together, which is an \"unsupervised learning task\" that groups data into \"subgroups or clusters based on similarity/dissimilarity\" (Kolhe SR. et al, 2010). Some of the methods we could use are: K-Means, K-Medoids, PAM and CLARA. These are all partition based algorithms (Gupta T. et al, 2018). Clustering allows for large amounts of data to be processed into a simpler result that can be later processed by machine learning or seen visually by the user.\n",
    "\n",
    "I will be using the K-Means clustering method which uses centroids as the mean point of a cluster, the data surrounding the centroids are grouped to the closest centroid by calculating the Euclidean distance between the two shortest points, the formula for this is:\n",
    "\n",
    "d = (($p_{1}$ - $q_{1}$)$^{2}$ + ($p_{2}$ - $q_{2}$)$^{2}$)$^{1/2}$\n",
    "\n",
    "The K-Means clustering algorithms works by first defining the number of centroids to use (the K value), this will also control how many clusters will be created. After K has been defined; randomly picked points matching the K value will be selected out of the dataset, these will be the centroids. Once they have been defined the Euclidean distance is used on all possible paths finding the shortest path for each piece of data. This will group the data related to the closest centroid forming clusters, however the centroids are not yet positioned as the average of the data. Therefore this method will then be repeated for all centroids until either two conditions are met:\n",
    "\n",
    "1. The centroids no longer move, indicating the data has been clustered correctly\n",
    "2. The max number of iterations allowed has been reached\n",
    "\n",
    "(Soni K.G and Patel A, 2017)\n",
    "\n",
    "To select the K value for the K-Means algorithm I will be using the Elbow Method, there are others that can be used such as \"Gap Statistic, Silhouette Coefficient, and Canopy\" (Yuan C and Yang H, 2019). The Elbow Method works by calculating the squared distances from each point to the centroid, giving us an inertia/distortion value. The optimum K value is found by looking at the point of inflection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775da37a-c47c-4624-8199-086ef8364629",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Method\n",
    "This section will show how I processed the data, used the elbow method and clustered it using KMeans in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4945169c-71bc-49c3-ae85-cc2452c35a37",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Libraries\n",
    "Before running any code some libary imports are required.\n",
    "\n",
    "- urlib.request is being used to download the data set\n",
    "- matplotlib is imported to allow for plotting the data onto a graph\n",
    "- pandas is being imported to allow for datasets to be imported and manipulated in-memory\n",
    "- sklearn is used to perform kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10b3c1-13da-4b72-9ee8-80288629bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7728e195-a4c2-4991-82c9-29cebfd4cfea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get Data\n",
    "Before doing any calculations we will need to download and prepare the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5bde2-0dc8-4ca9-af6d-9c6f6842ce80",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "This section of code allows for the dataset to be downloaded from source into the current working directory. The dataset will be downloaded from the \n",
    "\n",
    "When downloading the dataset a few extra white space characters are added so this code removes any by using the `strip()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4669455-fb9c-4620-a1cf-93fb949522ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "download_filename = \"iris.data\"\n",
    "\n",
    "with urlopen(source_url) as response:\n",
    "    content = response.read()\n",
    "    with open(download_filename, \"wb\") as fo:\n",
    "        fo.write(content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50fa6e-61bb-482d-a441-aec5325bb46c",
   "metadata": {},
   "source": [
    "### Process Data\n",
    "To use the downloaded dataset, it needs to be processed to be ready. It is missing the attribute headers so this code will add a header and write to the processed file path, so that it can be later loaded in as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29defef-9578-4780-a6f4-9e4d494b191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_filename = \"iris.csv\"\n",
    "csv_header = b\"sepal-length,sepal-width,petal-length,petal-width,class\"\n",
    "\n",
    "with open(download_filename, \"rb\") as src_fo:\n",
    "    with open(processed_filename, \"wb\") as fo:\n",
    "        fo.write(csv_header)\n",
    "        fo.write(b\"\\n\")\n",
    "        fo.write(src_fo.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad65efb8-d26b-47ff-be7d-d3e4d66a571f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Data From CSV Into DataFrame\n",
    "Once the dataset has been processed into a valid csv file, it can be loaded into a pandas DataFrame. This will allow for easier manipulation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5316d25-ab0d-4326-8632-00870641e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(processed_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046e3529-2cea-4855-a1ec-af8e4e86876c",
   "metadata": {},
   "source": [
    "## Show Loaded Data\n",
    "Once the data is loaded we can preview the dataset which currently shows all the columns from the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa98c0-7293-42d8-8c73-acef56411ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75040579-9ab3-4b1c-b0a3-fa17ddf617a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Select Columns To Use\n",
    "For clustering only two columns are needed, in this code sample they are selected and referenced in the variable `selected_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c6d315-16e7-4fa8-bd0b-00829e8ffdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"sepal-length\", \"petal-length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfaa321-b705-46de-a9e4-8d4bf98ebdfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_data = df[column_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf044b6a-e359-4535-8602-e518e0bf4a73",
   "metadata": {},
   "source": [
    "The `selected_data` can now be previewed, which shows the two selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52928d9-fb33-43af-ae4d-35bf583db024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499efa3-57f5-4bf7-a3b6-4266f91c1bde",
   "metadata": {},
   "source": [
    "## Preview Data\n",
    "Now the data is loaded it can be previewed without any clusters. This code will plot the selected columns onto a matplotlib scatter chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358adadc-5513-4b48-ad92-f26cd5e3ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data, x, y, xl, yl, title):\n",
    "    plt.scatter(x=x, y=y, data=data)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xl)\n",
    "    plt.ylabel(yl)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f46d41-f32a-4111-9559-d605e7705593",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = column_names\n",
    "plot_data(df, x, y, \"Sepal Length\", \"Petal Length\", \"Selected Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a7997-2bf6-4267-bac3-52e61093475c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Find Optimum K Value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2109b383-10ab-453e-9de0-2b0dfa1d0650",
   "metadata": {},
   "source": [
    "To find the optimum number for k we use an elbow plot. The \"elbow\" of the graph will indicate the k value, we can also go further along increasing the number of clusters as long as the inertia value is high enough, this however is highly dependant on the dataset that is being used.\n",
    "\n",
    "This code block will calculate KMeans until the `max_k` value is reached, then they will displayed on another matplotlib plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caacedd-39ad-4d89-a740-55dcdaaaeabb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_elbow_plot(data, max_k):\n",
    "    means = []\n",
    "    inertias = []\n",
    "    \n",
    "    for k in range(1,max_k):\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(data)\n",
    "        means.append(k)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(means, inertias, 'o-')\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Inertia\")\n",
    "    plt.title(\"Elbow Plot Of Possible K Values\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd13a4-8b41-4d05-a433-32ab0a077756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_elbow_plot(selected_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208fc022-d05e-4b40-8358-98ef01fa04f1",
   "metadata": {},
   "source": [
    "The elbow plot shows that the optimum number to use for k is two, which will mean there will be two clusters. We could also use three, any numbers after that will produce undesiged results due to the intertia value being low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbeb695-d3db-4efe-81dc-a2a0eda7fd2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculate KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce01bf-a8eb-49ee-ab77-81eb3ba59ac9",
   "metadata": {},
   "source": [
    "Once the optimum k value has been found, the selected data can be clustered using kmeans.\n",
    "\n",
    "In this section of code I have selected the min k value to be one and the max k value to be four, this is so that I can illustrate optimal and non-optimal clusters with different inertia values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f38938-7709-4ba0-b767-c6cdcc6e6eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k_min = 1\n",
    "k_max = 4\n",
    "cluster_columns = []\n",
    "\n",
    "for k in range(k_min, k_max + 1):\n",
    "    kmeans = KMeans(k).fit(selected_data)\n",
    "    df[f\"cluster_{k}\"] = kmeans.labels_\n",
    "    cluster_columns.append(f\"cluster_{k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e7119-2c7c-4fa0-82f4-e2da29e4c556",
   "metadata": {},
   "source": [
    "## Get Cluster Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a2f750-2c1f-4152-853c-24358c17f0fa",
   "metadata": {},
   "source": [
    "The dataset with the selected columns and clusters can now be previewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab5ad82-c7f1-4982-8b9a-ca4fa3b7898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[column_names + cluster_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52c757-c45d-47dd-b05d-2e9b9d33ff27",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot Clusters\n",
    "To plot the clusters using matplotlib, I have created a function that will draw one cluster with given data, then through the use of a for loop multiple clusters can be created and grouped using subplots.\n",
    "\n",
    "The `enumerate()` function will allow for us to iterate through the cluster columns and get back a step number, which is used to place the plot in it's correct place on screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7402d-0571-4c71-bf07-63e4fb63963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster(data, kmeans, x, y, c, xl, yl, title):\n",
    "    plt.scatter(x=x, y=y, c=c, data=data)\n",
    "    plt.scatter(\n",
    "        kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "        s=250, marker=\"x\", c=\"black\", label=\"centers\"\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xl)\n",
    "    plt.ylabel(yl)\n",
    "    plt.legend(scatterpoints=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e86f04f-05f4-4e96-837c-25e281a9fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,12))\n",
    "rows = len(cluster_columns) // 2\n",
    "for i, k_col in enumerate(cluster_columns, 1):\n",
    "    plt.subplot(rows, 2, i)\n",
    "    x, y = column_names\n",
    "    plot_cluster(df, kmeans, x, y, k_col, \"Sepal Length\", \"Petal Length\", k_col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947cf875-2ee0-4192-8e31-aac180d4dc95",
   "metadata": {},
   "source": [
    "# Results\n",
    "After generating the four clusters with K values of one, two, three and four we can see depending on the K value we get varying results.\n",
    "\n",
    "We can see that having only one cluster does not produce the desired results due to no clustering happening.\n",
    "\n",
    "Looking at the K value of two we can see that the elbow method has produced the correct outcome to find the optimum value for K, as the two clusters are clearly linearly separated as the two clusters are quite far apart.\n",
    "\n",
    "The plot with the K value of three, shows that the three species of iris's have been shown which is why this can also be a optimum K value.\n",
    "\n",
    "The last plot shows what a undesired K value (that being four) would look like when clustered, this produces too many clusters for data that are not distinct enough. If the K value was increased any higher it would just produce more clusters that were smaller but not actually different data, which is why the elbow method finds these to be undesired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f5c69-94e0-489a-a8e5-486d3e95c2ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "In conclusion we can see that the iris dataset was clustered so we could see the three species of plants.\n",
    "\n",
    "There are some issues with using KMeans that may happen when clustering other datasets that are not so distinct and well separated. The first issue is that the value K must be known before any clustering can be done. The second issue is overlapping data cannot be solved accurately. The third issue is due to Euclidean distance being used and that it can give uneven weights, it has been suggested that using city block distance or correlation distance may be more accurate (Chakraborty et al, 2020). The fourth issue is KMeans will only work for non-linear data The fifth issue is that the cluster centers are chosen randomly which can lead to the result being invalid (Gupta T. et al, 2018).\n",
    "\n",
    "We can see from our results some of these issues. We can see that even when using the optimum value for K found by the Elbow Method (that being two), we will get some data that is associated with the wrong cluster. We can also see the problem with not knowing the K value as the four plots generated with different K values show that the K value is highly important for producing a valid result.\n",
    "\n",
    "The reasons why we use K-Means despite having these issues is that it has less complex algorithms, it works on numeric values and it gives good results when data is distinct and linearly separable (ibid.).\n",
    "\n",
    "This can be seen in the plot with three clusters as each species of iris can be seen separated in each cluster.\n",
    "\n",
    "We could improve our clusters by using different clustering algorithms such as: K-Medoids, PAM and CLARA which may give better results, for example K-Medoids is similar to K-Means however after each iteration a new centroid is created from a centrally located point of data compared to just an average found in K-Means (ibid.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e4f0d-2376-4b66-97c8-775c9bdf90ab",
   "metadata": {},
   "source": [
    "# References\n",
    "- Britannica, T. Editors of Encyclopaedia (2021, September 22). iris. Encyclopedia Britannica. https://www.britannica.com/plant/Iris-plant-genus\n",
    "- Fisher, R.A., 1936. The use of multiple measurements in taxonomic problems. Annals of eugenics, 7(2), pp.179-188.\n",
    "- Bora, M., Jyoti, D., Gupta, D. and Kumar, A., 2014. Effect of different distance measures on the performance of K-means algorithm: an experimental study in Matlab. arXiv preprint arXiv:1405.7471.\n",
    "- Chakraborty, A., Faujdar, N., Punhani, A. and Saraswat, S., 2020, January. Comparative study of k-means clustering using iris data set for various distances. In 2020 10th International Conference on Cloud Computing, Data Science & Engineering (Confluence) (pp. 332-335). IEEE.\n",
    "- Kolhe, S.R. and Ranjana, S.Z., 2010. Clustering Iris Data using Supervised and Unsupervised Learning. International Journal of Computer Science and Application, (2010), pp.0974-0767.\n",
    "- Gupta, T. and Panda, S.P., 2018. A comparison of k-means clustering algorithm and clara clustering algorithm on iris dataset. International Journal of Engineering & Technology, 7(4), pp.4766-4768.\n",
    "- Soni, K.G. and Patel, A., 2017. Comparative Analysis of K-means and K-medoids Algorithm on IRIS Data. International Journal of Computational Intelligence Research, 13(5), pp.899-906.\n",
    "- Yuan, C. and Yang, H., 2019. Research on K-value selection method of K-means clustering algorithm. J, 2(2), pp.226-235.\n",
    "- Dua, D. and Graff, C. (2019). UCI Machine Learning Repository <https://archive.ics.uci.edu/ml/datasets/Iris>. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "- Andy McDonald, 2021. How to Use Unsupervised Clustering on Well Log Data with Python <https://towardsdatascience.com/how-to-use-unsupervised-learning-to-cluster-well-log-data-using-python-a552713748b5> (code licensed under MIT).\n",
    "- Python Software Foundation, no-date. Python 3 documentation. <https://docs.python.org/3/>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057decb8-ff91-49bb-b59e-564abd1155ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Appendices\n",
    "## Source Code\n",
    "- <https://github.com/enchanted-code/python-kmeans-clustering>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
